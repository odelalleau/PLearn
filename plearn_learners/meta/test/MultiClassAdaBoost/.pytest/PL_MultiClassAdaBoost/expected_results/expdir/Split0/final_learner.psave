*1 ->HyperLearner(
tester = *2 ->PTester(
expdir = "" ;
dataset = *3 ->MemoryVMatrix(
data = 200  6  [ 
0.337040679017727052 	0.412187421639805485 	0.000975192502189414778 	0.15342979927457695 	2.77555756156289135e-16 	1 	
0.837768842251384371 	0.629942465869766655 	0.99999866745689503 	0.978455821589285457 	1 	2 	
0.441028250703420444 	0.714550797205845578 	0.999661619121736456 	0.608818846025308336 	1 	2 	
0.722711173292159503 	0.534139299013230762 	0.997887018334026488 	0.886113226735210535 	0.999999999976055154 	2 	
0.299130871933929676 	0.360203732190537917 	4.5306261716615559e-05 	0.0928636905426252213 	0 	0 	
0.411124184726996522 	0.639982916836899007 	0.981226002279039111 	0.464305822137039914 	1 	2 	
0.710499249124032395 	0.539688826170337976 	0.997717138254322289 	0.875929972112166055 	0.999999999999153344 	2 	
0.184288240546414961 	0.473853871201329102 	0.000206568890665281835 	0.0439810027269273429 	6.28895564069864577e-07 	0 	
0.247835468384202062 	0.589217134980148494 	0.125253987172025016 	0.134643618374606644 	0.999999999999998446 	2 	
0.819995942533578437 	0.3742750365623283 	0.920000218196554864 	0.925438956030885773 	1.72362124573055553e-13 	2 	
0.208071391136063988 	0.288296895519905672 	1.48940319855128678e-07 	0.0271273696801949682 	0 	0 	
0.964567830275638527 	0.267751622669661571 	0.998437149798841572 	0.996322178190460095 	0 	2 	
0.622142023781279918 	0.569475125142184568 	0.994985677375217148 	0.781983006857181917 	0.999999999999999889 	2 	
0.656000195973205136 	0.632384490644939956 	0.999825304257418446 	0.86209851532548476 	1 	2 	
0.497917512041781807 	0.445255345050883378 	0.0961894574372425537 	0.441785210713063148 	0.00197616314790682868 	1 	
0.678670792025397485 	0.841421101756496204 	0.999999998654930278 	0.959419859808275932 	1 	2 	
0.283249432531651968 	0.394384255062699751 	0.000132176529349881111 	0.0922296303861396272 	0 	0 	
0.821939000754816407 	0.661650928488066725 	0.999999416552044829 	0.976559491191049567 	1 	2 	
0.623898818861622462 	0.568805675730366778 	0.995035619407568017 	0.783844059008265814 	1 	1 	
0.64844932725094484 	0.369018445347890467 	0.090859937004917124 	0.665359349749000017 	3.77475828372553224e-15 	1 	
0.617788016990077793 	0.845311267104432718 	0.999999996183034146 	0.934589218828132506 	1 	2 	
0.560568598210385627 	0.572297230594697615 	0.984167012020504606 	0.685246751326321069 	0.99999999999999889 	1 	
0.730797004413526241 	0.78620836832089469 	0.999999985005914471 	0.964466185212620175 	1 	2 	
0.634618032132718835 	0.750340665972830778 	0.999998947993276044 	0.900715501675833585 	1 	2 	
0.52095022666103985 	0.80575351668482309 	0.999999564090173987 	0.830723002874528271 	1 	2 	
0.787660374462927582 	0.642419269024499417 	0.999995935842380668 	0.96105907030885418 	1 	2 	
0.747573026163381504 	0.408777845324890643 	0.850478635167357888 	0.858465136763180037 	4.27088420185128825e-10 	1 	
0.344049451797056649 	0.59974194633533795 	0.693694777941641361 	0.292132388915243735 	0.999999999999863221 	1 	
0.380091470145261345 	0.390005285685505743 	0.000988058603141606095 	0.193680526820496524 	0 	0 	
0.136071444498306604 	0.174038733327655182 	1.67205693735184013e-11 	0.00519572415845664937 	0 	0 	
0.854708697955331553 	0.722448573065056387 	0.999999990058421329 	0.989025437968076515 	1 	2 	
0.757560278233593398 	0.483466030526050028 	0.993536629503178892 	0.901417965358118689 	0.99950050332745155 	2 	
0.883841283359871888 	0.51078332768391177 	0.999974531176691217 	0.983705356763069472 	0.999999999999489519 	2 	
0.499495000171716774 	0.128114914706039529 	4.6449742940879446e-09 	0.127761238240634289 	0 	0 	
0.518898492739471973 	0.278079094080102784 	0.000104947015177880854 	0.309516644117132977 	0 	0 	
0.65642431966618342 	0.694308507904370042 	0.999989248773869965 	0.892398673461820557 	1 	2 	
0.244436179936414477 	0.674302546821647364 	0.836802915551401627 	0.178239622821568466 	1 	1 	
0.231735044202631524 	0.68041073208306635 	0.826919421809612087 	0.162198541462586776 	1 	1 	
0.420495414774386123 	0.386383438501044674 	0.00196719004512274642 	0.249052878710770975 	0 	1 	
0.464482422012391649 	0.128947152459879366 	2.4809357901389717e-09 	0.100213915042869361 	0 	0 	
0.771956182942409086 	0.590668595471845581 	0.99994253517097742 	0.943001342964679301 	1 	2 	
0.324400846566160694 	0.392763663780197392 	0.000326988014224482981 	0.129919580057503758 	0 	0 	
0.104807823195401495 	0.662371410949871287 	0.0182405353402941284 	0.0261731357173424217 	1 	1 	
0.440594416537475053 	0.256177855855037051 	7.11662389785150395e-06 	0.176037206805503443 	0 	0 	
0.625115862267542122 	0.37115850993626881 	0.0620416273826774334 	0.621408291026117499 	1.66533453693773481e-16 	0 	
0.432416163989962576 	0.202030774074823449 	2.77740037468721113e-07 	0.12810151112620638 	0 	0 	
0.236708604174129145 	0.231879283006988346 	1.80263723947859944e-08 	0.0282227012152627688 	0 	0 	
0.847015190912564275 	0.465124225802565849 	0.99922322286709675 	0.963882434314954462 	0.992415987887875772 	2 	
0.538160645477002486 	0.737008943262299265 	0.999984420515098726 	0.791912130614169874 	1 	2 	
0.28333470835393415 	0.182425523806356349 	2.95443025599695375e-09 	0.0336709161573283122 	0 	0 	
0.587768965659763665 	0.547778771470676551 	0.975656839533297071 	0.711483510236805339 	0.999999999999949818 	2 	
0.618892475744320492 	0.332005146247901783 	0.0102816464864412538 	0.566975061872418307 	0 	1 	
0.694461302348206133 	0.609190310086851761 	0.99980542533631811 	0.88951435084880881 	1 	2 	
0.565943613014406988 	0.506652107479236169 	0.831002657098736996 	0.635906306170384461 	0.999955927919579879 	1 	
0.383086029073044121 	0.945550827907227998 	0.999999999995657696 	0.870359374800871999 	1 	2 	
0.735058297271116379 	0.533385253030013939 	0.998405580711887009 	0.897825170411419959 	0.999999999999998668 	2 	
0.247867159961008499 	0.587887685889858735 	0.11945303331244056 	0.134080683707188841 	0.999999999914290782 	1 	
0.31437592471957404 	0.748901091228292737 	0.999114931451879573 	0.385305232435436462 	1 	2 	
0.356049980032269731 	0.893799247488805126 	0.999999989146302282 	0.719902897371154649 	1 	2 	
0.234179480769041271 	0.370334983797080408 	1.32408575363451853e-05 	0.0521402176285982177 	0 	0 	
0.189684732456134986 	0.38424234704007143 	6.29266245716353367e-06 	0.0330792608751452888 	0 	0 	
0.207031917429434331 	0.565840436553556536 	0.0168649332947274355 	0.0816482575673892486 	0.99999999937393691 	1 	
0.489069241863633541 	0.385463930848654379 	0.00751776669233639172 	0.365124621784082615 	0 	1 	
0.554223418555835012 	0.262956169409198082 	9.92380307193729827e-05 	0.35573593061885489 	0 	0 	
0.546450670398301863 	0.275978125137457453 	0.000164348010774972852 	0.356545375200074566 	0 	0 	
0.331477526458701544 	0.544313426940081446 	0.150549210479969497 	0.226920877779638486 	0.999999973227484795 	1 	
0.608447255824831057 	0.576250504093414606 	0.994923430289251121 	0.766880537582550481 	1 	2 	
0.74833652534817019 	0.217437897918653533 	0.000637167891637702155 	0.710754238216330991 	0 	1 	
0.441918384856973989 	0.232118037868229266 	1.98305498017026238e-06 	0.159403906727846767 	0 	0 	
0.190465240239958922 	0.205912881869682318 	9.90857895732943916e-10 	0.014155296160479891 	0 	0 	
0.832796822890589872 	0.416922441027541113 	0.99074882064070191 	0.9466578043680558 	4.36913572099406622e-07 	1 	
0.328675871745063797 	0.276433934128551662 	1.86322469714061967e-06 	0.0840677020810715048 	0 	0 	
0.441644647089275755 	0.535400668942389912 	0.561201571146024536 	0.418897719404788704 	0.999999996397940238 	2 	
0.253670691389482728 	0.458786221981034303 	0.000868408106177720462 	0.0890735277355632071 	6.48091025290398193e-10 	1 	
0.585589013754027388 	0.224171518555312144 	2.28533497587668499e-05 	0.365583725331242504 	0 	0 	
0.58986806678244541 	0.301534783997461253 	0.00138193182218504518 	0.47202923266442659 	0 	1 	
0.85200702932755279 	0.811420997797132615 	0.999999999927318139 	0.993037708645332362 	1 	2 	
0.392618188076281682 	0.680723781534957939 	0.995455985334303017 	0.471184465788363038 	1 	2 	
0.293918116463156132 	0.507311978686203147 	0.016469475112039389 	0.151277035115703007 	0.0121899329063093198 	1 	
0.44097155133617183 	0.518326786127685679 	0.388719008958370693 	0.400848725095580616 	0.999915657472835395 	1 	
0.527938368925109258 	0.640300813976747207 	0.998214144045750151 	0.690179796975922688 	1 	2 	
0.303877709616286351 	0.433800927030910277 	0.00110349363209288898 	0.127343927610726104 	6.8489658389125907e-13 	0 	
0.413583708380048265 	0.495437626628920491 	0.126920024706036194 	0.328236931934144105 	0.4510101768362893 	2 	
0.371933728396660779 	0.493815315450534476 	0.053806134247652182 	0.254998228149233563 	0.020177763157040729 	1 	
0.42880705432535704 	0.710649379795884273 	0.99947513790175524 	0.580812579989998579 	1 	2 	
0.519185575742599603 	0.860101109434101785 	0.999999991171935276 	0.877510112077225846 	1 	2 	
0.433248369780109721 	0.603801463339838751 	0.9463565486409109 	0.470840625378211142 	1 	2 	
0.333090528816623599 	0.178629567229022346 	7.35518734806817065e-09 	0.0515043955812375387 	0 	0 	
0.363841364105409126 	0.420173792266473223 	0.00243767478559564488 	0.191508185938040587 	1.49880108324396133e-15 	1 	
0.258024822034749468 	0.778690243501383939 	0.999324390855314215 	0.298376672395714193 	1 	2 	
0.308444816500438312 	0.205530525293464994 	2.36985961965530123e-08 	0.0489708262894453794 	0 	0 	
0.275265359242249819 	0.356809252589806836 	2.18167656550471989e-05 	0.0740054232710109505 	0 	0 	
0.447978933902173193 	0.807225196990252192 	0.999998285967393041 	0.73374557424670217 	1 	2 	
0.569710475705847208 	0.124579530102164993 	1.386039960582508e-08 	0.200000697780095438 	0 	0 	
0.907073953054543303 	0.320080909610997666 	0.979327058711887077 	0.97818028340714902 	0 	1 	
0.346022306600591856 	0.33957471551374363 	5.35527421994252961e-05 	0.125863341080325264 	0 	0 	
0.603964125026318244 	0.633402136580338504 	0.999488949006740235 	0.800804485191756843 	1 	2 	
0.684720245521482473 	0.837664162630183062 	0.999999998453590444 	0.960523011225891343 	1 	2 	
0.708090895277180166 	0.471418691600586415 	0.963949563452032532 	0.840129240225992779 	0.000781299807628477172 	1 	
0.771582763339008881 	0.645452661865975386 	0.999994314239608828 	0.954011308682431336 	1 	2 	
0.460149701882857753 	0.332311270435153072 	0.000419402495029586042 	0.265641436326872871 	0 	0 	
0.831530044133463875 	0.529186781004250317 	0.999893949594790166 	0.964762122623453844 	0.999999999999998224 	2 	
0.65018898152607485 	0.387873429950195248 	0.187975328574094158 	0.686548020585021845 	1.37057032389975575e-13 	0 	
0.424831603024144355 	0.785299307796495372 	0.999989387209282654 	0.666209358798257378 	1 	2 	
0.223955933194476708 	0.85297906339534868 	0.9999884384030715 	0.325763342209897722 	1 	2 	
0.568189952611602878 	0.5042149001566536 	0.823608596003203242 	0.637266608512469634 	0.999735435267097072 	1 	
0.196164666090740902 	0.698362006304233507 	0.792958232215262515 	0.121199495873135221 	1 	2 	
0.635883152908142013 	0.739603738348431294 	0.999998198375502856 	0.896384266143035013 	1 	2 	
0.766440462928471122 	0.776889333222431144 	0.999999989972555126 	0.974032527455173658 	1 	2 	
0.439712238997702287 	0.223776970430579092 	1.18040709834454915e-06 	0.150663062771468848 	0 	0 	
0.270576426620127486 	0.800109679656329731 	0.999865169866101078 	0.355412841484316178 	1 	2 	
0.595406137160540805 	0.273123998909772792 	0.000387076909607542152 	0.44840679059006272 	0 	1 	
0.709583071638395624 	0.244892645065264869 	0.00111965991489298977 	0.659388419418811988 	0 	0 	
0.631612337149145886 	0.741398499859989402 	0.999998200949979932 	0.893976621243775638 	1 	2 	
0.694771004175420592 	0.532118213643730242 	0.995500010183678841 	0.854748431601624392 	0.99999999999676481 	2 	
0.690109228104916594 	0.857407207850628961 	0.999999999704517029 	0.967619107040243787 	1 	2 	
0.791478407262177441 	0.706123387877502617 	0.999999802130912219 	0.971923386203978446 	1 	2 	
0.591951822229620506 	0.447380558507254611 	0.437225483940870852 	0.630336343554293577 	3.58298156311054328e-07 	1 	
0.416691516057121847 	0.536761047080265952 	0.448011082314667908 	0.371359256055882048 	0.999999999896008296 	2 	
0.241271710605322209 	0.574171145689825524 	0.0606744175626963145 	0.120029588541164689 	0.999999999998035904 	1 	
0.61683834213865274 	0.786651776820837867 	0.999999800892349078 	0.905214887115931233 	1 	2 	
0.893793552025074045 	0.56782679783496337 	0.999998454853059715 	0.989364338420214429 	1 	2 	
0.65921321328677962 	0.755683846757965627 	0.99999953936066821 	0.920540241090206934 	1 	2 	
0.403657119741667958 	0.475973518743775081 	0.0515078153070945288 	0.293677438926866541 	2.50858883715232572e-05 	1 	
0.183188899325371801 	0.79680275397946343 	0.997954101035577246 	0.164779162168442184 	1 	2 	
0.159915087742074802 	0.641405583127205969 	0.0773014406994857461 	0.0607616755376079176 	1 	2 	
0.590438885457506224 	0.42420618845194985 	0.226805116304403587 	0.604891812518721261 	2.96487784323673509e-09 	1 	
0.2739605965980349 	0.488614468492700071 	0.00482695885510686651 	0.119748916573926278 	4.1677502848891379e-06 	2 	
0.265423008491722512 	0.573310286606669717 	0.105633508560364686 	0.149217695648006854 	0.999999999999996669 	1 	
0.260515685218742687 	0.730119706711473304 	0.9913015161315889 	0.25152413682623348 	1 	2 	
0.859460949480679082 	0.482986570768417611 	0.999769089631301733 	0.972226055181255377 	0.999999927521804555 	2 	
0.715253965469259345 	0.501222497563960645 	0.990567622975351059 	0.863734471591099506 	0.999999956288300407 	2 	
0.705809945524337068 	0.544043054318400721 	0.997853911496872081 	0.872929195401613289 	0.999999999703498843 	2 	
0.384680150978340629 	0.709568646181242491 	0.998620053983448219 	0.488109742125673263 	1 	2 	
0.477595014000419482 	0.277766030791077922 	4.52085102251031934e-05 	0.243125441624808647 	0 	0 	
0.363834420308997264 	0.713118834028081405 	0.998188849124578059 	0.448461205598444546 	1 	2 	
0.80244558809579325 	0.420965787314543094 	0.97854438996804638 	0.923014652663194157 	3.32325866825300409e-09 	1 	
0.472425506246140436 	0.761427188712762382 	0.999984163384094682 	0.718962280122859676 	1 	2 	
0.386511475642943525 	0.584620412924011124 	0.751705671104142548 	0.358433429691765726 	0.999999999999998224 	1 	
0.313344389534551349 	0.563876550224456041 	0.205293859180526783 	0.212465091231545666 	0.999999850909174426 	2 	
0.437498628835231718 	0.271487240160874554 	1.47010049714180191e-05 	0.18408839079913808 	0 	0 	
0.349972520410344767 	0.823272754127770234 	0.999995406942313592 	0.574459628804300237 	1 	2 	
0.511535452700446203 	0.163949577360044452 	1.05913818548852845e-07 	0.177025707750321615 	0 	0 	
0.333981910556185979 	0.777083405190116761 	0.999881013318794643 	0.467065256689115749 	1 	2 	
0.733563847159287241 	0.523964125199950947 	0.997583968485294292 	0.89304320560242656 	0.999999987101309107 	1 	
0.664233998157672811 	0.223942029527671616 	0.000121280891147379499 	0.530026326836644524 	0 	0 	
0.747159532501805002 	0.61025399546998349 	0.999949900528595004 	0.931935003882062851 	1 	2 	
0.647068916766090241 	0.842445826312409496 	0.999999997473194568 	0.947256531484346631 	1 	2 	
0.620160612968468916 	0.293360169711035401 	0.00176105431478684737 	0.525328301669363729 	0 	1 	
0.180912118693824064 	0.473026213425085496 	0.000178464923893839611 	0.0419870523418195973 	2.19526119504820372e-10 	0 	
0.171486298949319993 	0.453303976992695201 	5.83472488787095678e-05 	0.034303904575232691 	2.83106871279414918e-14 	1 	
0.480642180523963292 	0.496247570451631481 	0.368762354055697128 	0.457569793089636412 	0.0782792313851559673 	1 	
0.460646565225021232 	0.78734912020993153 	0.99999545505324261 	0.729662318511505426 	1 	2 	
0.467164824120552491 	0.375671870776489891 	0.00321294243586323081 	0.316254176329649583 	0 	0 	
0.43385005121768927 	0.85176982124301015 	0.99999990359908042 	0.771436448839911471 	1 	2 	
0.537295599170006577 	0.625186163434580333 	0.997167078991974876 	0.692086063473239976 	1 	1 	
0.77206002977913113 	0.408222101578465835 	0.915805160160764076 	0.887859404382179029 	5.1289749669614082e-09 	2 	
0.215260988690660682 	0.421377785818776385 	6.51462934598834309e-05 	0.0518779202071764645 	5.5511151231257827e-17 	0 	
0.526240868080626534 	0.344837501105276723 	0.00275164754263412803 	0.393932927643954378 	0 	1 	
0.352332982714121834 	0.228780499937454418 	2.5141515719306895e-07 	0.0807752533440048159 	0 	0 	
0.530556872338403984 	0.435769357914573607 	0.122211770968852884 	0.497170030219862114 	5.83355058703105556e-09 	0 	
0.418968313566468575 	0.676882136091753517 	0.996857941138302728 	0.521337033709274533 	1 	1 	
0.403587313458172448 	0.824644927680108442 	0.999998667856597523 	0.682776283308621723 	1 	2 	
0.361998630782866759 	0.370204408571073618 	0.00028952770396439842 	0.159220420181713229 	0 	1 	
0.908979683193673837 	0.839935736059859939 	0.999999999999364064 	0.99809217160669339 	1 	2 	
0.832844161267142113 	0.489798915748868746 	0.999510418543040791 	0.959708536196156592 	0.999999999975029863 	2 	
0.247191042132576255 	0.565702859063909158 	0.0509387213597051192 	0.123082290071373113 	0.999999999999863887 	1 	
0.236347439196011755 	0.4184425886375715 	0.000105586969146143073 	0.064450896143266434 	1.17683640610266593e-14 	0 	
0.878774939360102647 	0.250912786885272465 	0.262483546060253647 	0.946270181212051198 	0 	1 	
0.560856785545585934 	0.387939911774027868 	0.0343357208576768325 	0.507874348900542727 	3.68705066478014487e-13 	1 	
0.61900332590026208 	0.209175906866663652 	1.89742269794956897e-05 	0.411150816107167638 	0 	0 	
0.198155916735463955 	0.72655228198141264 	0.941735419830920351 	0.13952389135164156 	1 	2 	
0.869351838403421562 	0.6676266001109048 	0.999999928312545139 	0.988892993459312253 	1 	2 	
0.314412568305269202 	0.379561463986388015 	0.000148908442682282871 	0.114100891594720921 	0 	0 	
0.670922423671947432 	0.408098903170796223 	0.460947205585807318 	0.741358235198477655 	2.63092586605395695e-09 	2 	
0.307845747776586576 	0.107073381074912766 	1.06973874203220021e-11 	0.0231603669418962155 	0 	0 	
0.764897455283813699 	0.67360817009344176 	0.99999804306670792 	0.956255113223712705 	1 	2 	
0.602099322710622831 	0.705791257374221992 	0.999980034404002849 	0.845851840468065497 	1 	2 	
0.498410005359550323 	0.443684957890750631 	0.0916210989854581159 	0.440416965031909857 	9.98667815110820811e-12 	0 	
0.2494982653002355 	0.551907308225648374 	0.0315959640562301081 	0.119655134207207636 	0.999999866082263766 	1 	
0.371349918041573357 	0.649128355496379927 	0.971250387171610163 	0.392262925955319308 	1 	2 	
0.676505835838878578 	0.232669252996851095 	0.000262714478394654449 	0.569629577497201667 	0 	0 	
0.630375954790908244 	0.224279366557609772 	5.88853110093379151e-05 	0.45672355544882115 	0 	0 	
0.300711932385879122 	0.149829364552470645 	4.24941304366655004e-10 	0.031597799967373108 	0 	0 	
0.584741827477265108 	0.0924290853768408027 	6.64550636741978451e-10 	0.168118627952250577 	0 	0 	
0.201814833111637948 	0.540414142004595033 	0.00519546572841989018 	0.0699693665010114874 	0.999805662526560335 	1 	
0.558190128039561695 	0.616932877239985022 	0.997360703265658066 	0.720076277306989421 	1 	2 	
0.470688646750369089 	0.210093817686775486 	9.85101667738685904e-07 	0.173895030928417071 	0 	0 	
0.532031073717923908 	0.760036567480947722 	0.999994818185025225 	0.803668134940642487 	1 	2 	
0.208994001596146228 	0.76073439080480787 	0.992696853128328893 	0.18157395531864462 	1 	2 	
0.116346727192441657 	0.167537635143858665 	4.31321645066873316e-12 	0.0034723336958780715 	0 	0 	
0.134669253672196843 	0.830249986777758409 	0.998603683097191275 	0.105850998399096619 	1 	2 	
0.549769642170096162 	0.552231256528370573 	0.95671254064267286 	0.647573451897092478 	0.999840819088372856 	2 	
0.165243489232508756 	0.537479564772721186 	0.0013631676983125729 	0.0434812741523764035 	0.925412731147525536 	1 	
0.317949746542236178 	0.74753339550780562 	0.999123747824000263 	0.3910073121171318 	1 	2 	
0.392801824493781482 	0.487425152233795955 	0.0641128554671341333 	0.284365332967790707 	0.994211403487852863 	1 	
0.710161800997805237 	0.271536688465156772 	0.00455234525237191434 	0.690991769450284621 	0 	0 	
0.258779846056689244 	0.328034106346786924 	3.98635217790932472e-06 	0.0561383721754414866 	0 	0 	
0.606341601082149295 	0.530905550153722428 	0.967631950849599187 	0.728364397980203471 	0.999999999996517897 	0 	
0.600135566392399955 	0.291099509296859849 	0.00103701637983616424 	0.48018039043485411 	0 	0 	
]
;
fieldinfos = 6 [ "x1" 0 "x2" 0 "x3" 0 "x4" 0 "y1" 0 "target" 0 ] ;
fieldnames = []
;
deep_copy_memory_data = 1 ;
writable = 0 ;
length = 200 ;
width = 6 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
extrasize = 0 ;
metadatadir = "" ;
source = *0  )
;
splitter = *4 ->FractionSplitter(
round_to_closest = 0 ;
splits = 1  3  [ 
(0 , 0.75 )	(0 , 0.75 )	(0.75 , 1 )	
]
;
one_is_absolute = 0  )
;
statnames = 8 [ "E[test1.E[class_error]]" "E[test1.E[linear_class_error]]" "E[test1.E[square_class_error]]" "E[test1.E[conflict]]" "E[test2.E[class_error]]" "E[test2.E[linear_class_error]]" "E[test2.E[square_class_error]]" "E[test2.E[conflict]]" ] ;
statmask = []
;
learner = *5 ->MultiClassAdaBoost(
random_gen = *0 ;
seed = 1827 ;
stage = 1 ;
n_examples = 150 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 1 ;
report_progress = 1 ;
verbosity = 1 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827 ;
finalized = 0 ;
learner1 = *6 ->AdaBoost(
weak_learners = 1 [ *7 ->RegressionTree(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
maximum_number_of_nodes = 4 ;
compute_train_stats = 0 ;
complexity_penalty_factor = 0 ;
output_confidence_target = 0 ;
multiclass_outputs = 3 [ 0 1 2 ] ;
leave_template = *8 ->RegressionTreeLeave(
id = 0 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
root = *9 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 1 1 ] ;
leave_error = 3 [ 0.378311111111107712 0 0.378311111111107712 ] ;
split_col = 2 ;
split_balance = 70 ;
split_feature_value = 0.00125079586853901747 ;
after_split_error = 0.0741818181818175992 ;
missing_node = *0 ;
missing_leave = *10 ->RegressionTreeLeave(
id = 2 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *11 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0.04799999999999989 0 0.04799999999999989 ] ;
split_col = 2 ;
split_balance = 24 ;
split_feature_value = 0.000357032461916012567 ;
after_split_error = 0.0266666666666666094 ;
missing_node = *0 ;
missing_leave = *12 ->RegressionTreeLeave(
id = 5 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *13 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 3 ;
split_balance = 0 ;
split_feature_value = 0.113038628061597313 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *14 ->RegressionTreeLeave(
id = 11 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *0 ;
left_leave = *0 ;
right_node = *0 ;
right_leave = *0  )
;
left_leave = *0 ;
right_node = *15 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0.0266666666666666094 0 0.0266666666666666094 ] ;
split_col = 2 ;
split_balance = 2 ;
split_feature_value = 0.000981625552665510437 ;
after_split_error = 0.0106666666666666438 ;
missing_node = *0 ;
missing_leave = *16 ->RegressionTreeLeave(
id = 14 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *17 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 1 1 ] ;
leave_error = 3 [ 0.0106666666666666438 0 0.0106666666666666438 ] ;
split_col = 2 ;
split_balance = 1 ;
split_feature_value = 0.000528285193333644099 ;
after_split_error = 0.00666666666666665235 ;
missing_node = *0 ;
missing_leave = *18 ->RegressionTreeLeave(
id = 17 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *0 ;
left_leave = *0 ;
right_node = *0 ;
right_leave = *0  )
;
left_leave = *0 ;
right_node = *19 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0 0 0 ] ;
split_col = 4 ;
split_balance = 1 ;
split_feature_value = 3.42448291945629535e-13 ;
after_split_error = 0 ;
missing_node = *0 ;
missing_leave = *20 ->RegressionTreeLeave(
id = 20 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *0 ;
left_leave = *0 ;
right_node = *0 ;
right_leave = *0  )
;
right_leave = *0  )
;
right_leave = *0  )
;
left_leave = *0 ;
right_node = *21 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 1 1 ] ;
leave_error = 3 [ 0.0261818181818178619 0 0.0261818181818178619 ] ;
split_col = 4 ;
split_balance = 88 ;
split_feature_value = 1.54709578481515564e-13 ;
after_split_error = 0.0218181818181815007 ;
missing_node = *0 ;
missing_leave = *22 ->RegressionTreeLeave(
id = 8 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *0 ;
left_leave = *0 ;
right_node = *0 ;
right_leave = *0  )
;
right_leave = *0  )
;
priority_queue = *0 ;
first_leave = *0 ;
split_cols = []
;
split_values = []
;
random_gen = *0 ;
seed = 1827 ;
stage = 4 ;
n_examples = 150 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 1 ;
forget_when_training_set_changes = 1 ;
nstages = 4 ;
report_progress = 1 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827 ;
finalized = 1  )
] ;
voting_weights = 1 [ 1.94591014905531456 ] ;
sum_voting_weights = 1.94591014905531456 ;
initial_sum_weights = 1.00000000000000244 ;
example_weights = 150 [ 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.166666666666667074 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.166666666666667074 0.00340136054421768695 0.166666666666667074 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 0.00340136054421768695 ] ;
learners_error = 1 [ 0.0199999999999999518 ] ;
weak_learner_template = *23 ->RegressionTree(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
maximum_number_of_nodes = 3 ;
compute_train_stats = 0 ;
complexity_penalty_factor = 0 ;
output_confidence_target = 0 ;
multiclass_outputs = 3 [ 0 1 2 ] ;
leave_template = *8  ;
root = *0 ;
priority_queue = *0 ;
first_leave = *0 ;
split_cols = []
;
split_values = []
;
random_gen = *0 ;
seed = 1827 ;
stage = 0 ;
n_examples = 200 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 1 ;
forget_when_training_set_changes = 1 ;
nstages = 4 ;
report_progress = 1 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827 ;
finalized = 0  )
;
target_error = 0.5 ;
pseudo_loss_adaboost = 1 ;
conf_rated_adaboost = 0 ;
weight_by_resampling = 0 ;
output_threshold = 0.5 ;
provide_learner_expdir = 1 ;
early_stopping = 0 ;
save_often = 0 ;
compute_training_error = 0 ;
forward_sub_learner_test_costs = 1 ;
modif_train_set_weights = 1 ;
found_zero_error_weak_learner = 0 ;
reuse_test_results = 0 ;
random_gen = *0 ;
seed = 1827 ;
stage = 1 ;
n_examples = 150 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 1 ;
forget_when_training_set_changes = 0 ;
nstages = 1 ;
report_progress = 1 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827 ;
finalized = 0  )
;
learner2 = *24 ->AdaBoost(
weak_learners = 1 [ *25 ->RegressionTree(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
maximum_number_of_nodes = 4 ;
compute_train_stats = 0 ;
complexity_penalty_factor = 0 ;
output_confidence_target = 0 ;
multiclass_outputs = 3 [ 0 1 2 ] ;
leave_template = *8  ;
root = *26 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0.499911111111108142 0 0.499911111111108142 ] ;
split_col = 2 ;
split_balance = 24 ;
split_feature_value = 0.991025168386145405 ;
after_split_error = 0.173253056011676232 ;
missing_node = *0 ;
missing_leave = *27 ->RegressionTreeLeave(
id = 2 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *28 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0.147432950191570544 0 0.147432950191570544 ] ;
split_col = 1 ;
split_balance = 31 ;
split_feature_value = 0.482293993618237549 ;
after_split_error = 0.104535916061339579 ;
missing_node = *0 ;
missing_leave = *29 ->RegressionTreeLeave(
id = 5 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *30 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0.0131073446327683307 0 0.0131073446327683307 ] ;
split_col = 3 ;
split_balance = 53 ;
split_feature_value = 0.924226804347039965 ;
after_split_error = 0.00888888888888886806 ;
missing_node = *0 ;
missing_leave = *31 ->RegressionTreeLeave(
id = 11 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *0 ;
left_leave = *0 ;
right_node = *0 ;
right_leave = *0  )
;
left_leave = *0 ;
right_node = *32 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0.0914285714285712481 0 0.0914285714285712481 ] ;
split_col = 2 ;
split_balance = 18 ;
split_feature_value = 0.891579732096156263 ;
after_split_error = 0.0802318840579708398 ;
missing_node = *0 ;
missing_leave = *33 ->RegressionTreeLeave(
id = 14 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *34 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 0 1 ] ;
leave_error = 3 [ 0.069565217391304196 0 0.069565217391304196 ] ;
split_col = 2 ;
split_balance = 15 ;
split_feature_value = 0.808283414109232878 ;
after_split_error = 0.0617543859649121521 ;
missing_node = *0 ;
missing_leave = *35 ->RegressionTreeLeave(
id = 17 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *0 ;
left_leave = *0 ;
right_node = *0 ;
right_leave = *0  )
;
left_leave = *0 ;
right_node = *36 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 1 1 ] ;
leave_error = 3 [ 0.0106666666666666438 0 0.0106666666666666438 ] ;
split_col = 2 ;
split_balance = 1 ;
split_feature_value = 0.982696507149771858 ;
after_split_error = 0.00666666666666665061 ;
missing_node = *0 ;
missing_leave = *37 ->RegressionTreeLeave(
id = 20 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *0 ;
left_leave = *0 ;
right_node = *0 ;
right_leave = *0  )
;
right_leave = *0  )
;
right_leave = *0  )
;
left_leave = *0 ;
right_node = *38 ->RegressionTreeNode(
missing_is_valid = 0 ;
leave = *0 ;
leave_output = 2 [ 1 1 ] ;
leave_error = 3 [ 0.0258201058201057432 0 0.0258201058201057432 ] ;
split_col = 2 ;
split_balance = 47 ;
split_feature_value = 0.997650553369808346 ;
after_split_error = 0.0199999999999999553 ;
missing_node = *0 ;
missing_leave = *39 ->RegressionTreeLeave(
id = 8 ;
missing_leave = 0 ;
loss_function_weight = 1 ;
verbosity = 2 ;
length = 0 ;
weights_sum = 0 ;
targets_sum = 0 ;
weighted_targets_sum = 0 ;
weighted_squared_targets_sum = 0 ;
loss_function_factor = 2 ;
output_confidence_target = 1  )
;
left_node = *0 ;
left_leave = *0 ;
right_node = *0 ;
right_leave = *0  )
;
right_leave = *0  )
;
priority_queue = *0 ;
first_leave = *0 ;
split_cols = []
;
split_values = []
;
random_gen = *0 ;
seed = 1827 ;
stage = 4 ;
n_examples = 150 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 1 ;
forget_when_training_set_changes = 1 ;
nstages = 4 ;
report_progress = 1 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827 ;
finalized = 1  )
] ;
voting_weights = 1 [ 1.22117351768460325 ] ;
sum_voting_weights = 1.22117351768460325 ;
initial_sum_weights = 1.00000000000000244 ;
example_weights = 150 [ 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.0416666666666667962 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 0.00362318840579710496 ] ;
learners_error = 1 [ 0.0799999999999998351 ] ;
weak_learner_template = *40 ->RegressionTree(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
maximum_number_of_nodes = 3 ;
compute_train_stats = 0 ;
complexity_penalty_factor = 0 ;
output_confidence_target = 0 ;
multiclass_outputs = 3 [ 0 1 2 ] ;
leave_template = *8  ;
root = *0 ;
priority_queue = *0 ;
first_leave = *0 ;
split_cols = []
;
split_values = []
;
random_gen = *0 ;
seed = 1827 ;
stage = 0 ;
n_examples = 200 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 1 ;
forget_when_training_set_changes = 1 ;
nstages = 4 ;
report_progress = 1 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827 ;
finalized = 0  )
;
target_error = 0.5 ;
pseudo_loss_adaboost = 1 ;
conf_rated_adaboost = 0 ;
weight_by_resampling = 0 ;
output_threshold = 0.5 ;
provide_learner_expdir = 1 ;
early_stopping = 0 ;
save_often = 0 ;
compute_training_error = 0 ;
forward_sub_learner_test_costs = 1 ;
modif_train_set_weights = 1 ;
found_zero_error_weak_learner = 0 ;
reuse_test_results = 0 ;
random_gen = *0 ;
seed = 1827 ;
stage = 1 ;
n_examples = 150 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 1 ;
forget_when_training_set_changes = 0 ;
nstages = 1 ;
report_progress = 1 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827 ;
finalized = 0  )
;
forward_sub_learner_test_costs = 1 ;
learner_template = *41 ->AdaBoost(
weak_learners = []
;
voting_weights = []
;
sum_voting_weights = 0 ;
initial_sum_weights = 0 ;
example_weights = []
;
learners_error = []
;
weak_learner_template = *42 ->RegressionTree(
missing_is_valid = 0 ;
loss_function_weight = 1 ;
maximum_number_of_nodes = 3 ;
compute_train_stats = 0 ;
complexity_penalty_factor = 0 ;
output_confidence_target = 0 ;
multiclass_outputs = 3 [ 0 1 2 ] ;
leave_template = *8  ;
root = *0 ;
priority_queue = *0 ;
first_leave = *0 ;
split_cols = []
;
split_values = []
;
random_gen = *0 ;
seed = 1827 ;
stage = 0 ;
n_examples = -1 ;
inputsize = -1 ;
targetsize = -1 ;
weightsize = -1 ;
forget_when_training_set_changes = 1 ;
nstages = 4 ;
report_progress = 1 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827 ;
finalized = 0  )
;
target_error = 0.5 ;
pseudo_loss_adaboost = 1 ;
conf_rated_adaboost = 0 ;
weight_by_resampling = 0 ;
output_threshold = 0.5 ;
provide_learner_expdir = 1 ;
early_stopping = 0 ;
save_often = 0 ;
compute_training_error = 0 ;
forward_sub_learner_test_costs = 1 ;
modif_train_set_weights = 0 ;
found_zero_error_weak_learner = 0 ;
reuse_test_results = 0 ;
random_gen = *0 ;
seed = 1827 ;
stage = 0 ;
n_examples = -1 ;
inputsize = -1 ;
targetsize = -1 ;
weightsize = -1 ;
forget_when_training_set_changes = 0 ;
nstages = 1 ;
report_progress = 1 ;
verbosity = 2 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827 ;
finalized = 0  )
;
forward_test = 0 ;
time_costs = 0 ;
warn_once_target_gt_2 = 0  )
;
perf_evaluators = {};
report_stats = 1 ;
save_initial_tester = 0 ;
save_stat_collectors = 1 ;
save_split_stats = 1 ;
save_learners = 0 ;
save_initial_learners = 0 ;
save_data_sets = 0 ;
save_test_outputs = 0 ;
call_forget_in_run = 1 ;
save_test_costs = 0 ;
save_test_names = 0 ;
provide_learner_expdir = 1 ;
should_train = 1 ;
should_test = 1 ;
finalize_learner = 0 ;
template_stats_collector = *0 ;
global_template_stats_collector = *0 ;
final_commands = []
;
save_test_confidence = 0 ;
enforce_clean_expdir = 1 ;
redirect_stdout = 0 ;
redirect_stderr = 0  )
;
option_fields = 1 [ "nstages" ] ;
dont_restart_upon_change = 1 [ "nstages" ] ;
strategy = 1 [ *43 ->HyperOptimize(
which_cost = "E[test2.E[class_error]]" ;
min_n_trials = 0 ;
oracle = *44 ->EarlyStoppingOracle(
option = "nstages" ;
values = []
;
range = 3 [ 1 21 1 ] ;
min_value = -3.40282000000000014e+38 ;
max_value = 3.40282000000000014e+38 ;
max_degradation = 3.40282000000000014e+38 ;
relative_max_degradation = -1 ;
min_improvement = -3.40282000000000014e+38 ;
relative_min_improvement = -1 ;
max_degraded_steps = 120 ;
min_n_steps = 2 ;
nreturned = 20 ;
best_objective = 0.220000000000000001 ;
best_step = 2 ;
met_early_stopping = 0  )
;
provide_tester_expdir = 0 ;
sub_strategy = []
;
rerun_after_sub = 0 ;
provide_sub_expdir = 1 ;
save_best_learner = 0 ;
splitter = *0 ;
auto_save = 0 ;
auto_save_diff_time = 10800 ;
auto_save_test = 0 ;
best_objective = 0.220000000000000001 ;
best_results = 8 [ 0.100000000000000006 0.100000000000000006 0.100000000000000006 0 0.220000000000000001 0.239999999999999991 0.280000000000000027 0 ] ;
best_learner = *5  ;
trialnum = 20 ;
option_vals = []
;
verbosity = 0  )
] ;
provide_strategy_expdir = 1 ;
save_final_learner = 0 ;
finalize_learner = 0 ;
learner = *5  ;
provide_learner_expdir = 1 ;
expdir_append = "" ;
forward_nstages = 0 ;
random_gen = *0 ;
stage = 1 ;
n_examples = 200 ;
inputsize = 5 ;
targetsize = 1 ;
weightsize = 0 ;
forget_when_training_set_changes = 0 ;
nstages = 1 ;
report_progress = 1 ;
verbosity = 1 ;
nservers = 0 ;
save_trainingset_prefix = "" ;
test_minibatch_size = 1 ;
use_a_separate_random_generator_for_testing = 1827 ;
finalized = 0  )
